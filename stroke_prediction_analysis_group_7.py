# -*- coding: utf-8 -*-
"""Stroke_Prediction_Analysis_Group_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-zoKOa9eoUheWj0Sgpdh3COjs88JH25
"""

pip install eli5

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay, RocCurveDisplay
from eli5.sklearn import PermutationImportance
from eli5 import show_weights

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.inspection import PartialDependenceDisplay
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from xgboost import XGBClassifier

import joblib

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/healthcare-dataset-stroke-data.csv') #upload the dataset on the drive, mount drive, copy path of dataset
df.head()

# Printing the number of N/A values in eacg column
print(df.isna().sum())
# Graphical representation of the na values present in the attribute - bar graph
df.isna().sum().plot.barh()

# To check the  statistical analysis of all numerical type attributes  (count, mean, standaard deviation, minimum values, all quartiles, maximum values)
df.describe()

# Provides the data type of all attributes and the number of NOT NULL values count is obtained
df.info()

# Converting numeric-binary value attributes to string
# df[['hypertension', 'heart_disease', 'stroke']] = df[['hypertension', 'heart_disease', 'stroke']].astype(str)
# Generating dummy attributes - one hot encoding format
missing_values = df[['age', 'avg_glucose_level', 'bmi']].isnull().sum()

# If there are missing values, you need to handle them (e.g., by filling with a specific value)
if missing_values.any():
    # Fill missing values with a specific value (you can choose a different strategy based on your data)
    df[['age', 'avg_glucose_level', 'bmi']] = df[['age', 'avg_glucose_level', 'bmi']].fillna(0)
df = pd.get_dummies(df, drop_first= True)
df['age'] = df['age'].astype(int)
df['avg_glucose_level'] = df['avg_glucose_level'].astype(int)
df['bmi'] = df['bmi'].astype(int)

# The data frame after performing dummy attributes
df.head()

df['stroke'].value_counts().plot(kind="pie")
plt.title('Stroke in people before SMOTE')

# Since our Dataset is highly undersampled (based on target instances) we are going to perform a over sampling method to have equal representation of both the target classes
# Using random oversampling - importing the library
from imblearn.over_sampling import RandomOverSampler

# Performing a minority oversampling
oversample = RandomOverSampler(sampling_strategy='minority')
X=df.drop(['stroke'],axis=1)
y=df['stroke']

# Obtaining the oversampled dataframes - testing and training
X_over, y_over = oversample.fit_resample(X, y)

plt.title('Stroke in people after SMOTE')
pd.Series(y_over).value_counts().plot(kind="pie")

# Converting numeric-binary value attributes to string
# df[['hypertension', 'heart_disease', 'stroke']] = df[['hypertension', 'heart_disease', 'stroke']].astype(str)
# # Generating dummy attributes - one hot encoding format
# df = pd.get_dummies(df, drop_first= True)

X_over = X_over.drop(['id'],axis=1)
X_over.head()

standard_scalar=StandardScaler()
normalized_data=standard_scalar.fit_transform(X_over)

x_train,x_test,y_train,y_test=train_test_split(normalized_data,y_over,shuffle=True)

mlp_cls=Sequential([
    Dense(units=512,activation='relu',input_dim=x_train.shape[1]),
    Dropout(0.1),
    Dense(units=256,activation='relu'),
    Dropout(0.1),
    Dense(units=128,activation='relu'),
    Dropout(0.1),
    Dense(units=64,activation='relu'),
    Dropout(0.1),
    Dense(units=1,activation='sigmoid')
])

mlp_cls.summary()

x2_train, x2_val, y2_train, y2_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True)
erl_stpg=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)

mlp_cls.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
mlp_cls.fit(x2_train,y2_train,epochs=100,batch_size=256,validation_data=(x2_val,y2_val),callbacks=[erl_stpg])

xgb_cls=XGBClassifier()

xgb_prm_grd = {
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
}

xgb_grd_srch = GridSearchCV(estimator=xgb_cls, param_grid=xgb_prm_grd, scoring='neg_log_loss', cv=5)
xgb_grd_srch.fit(x_train, y_train)

xgb_bst_prms = xgb_grd_srch.best_params_
print("Best Hyperparameters:", xgb_bst_prms)
xgb_cls_bst=xgb_grd_srch.best_estimator_

logistic_regression_classifer=LogisticRegression()
logistic_regression_classifer.fit(x_train,y_train)

knn_classifier=KNeighborsClassifier()

knn_classifier_param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'p': [1, 2]
}

knn_classifier_grid_search = GridSearchCV(estimator=knn_classifier, param_grid=knn_classifier_param_grid, scoring='neg_log_loss', cv=5)
knn_classifier_grid_search.fit(x_train,y_train)

knn_classifier_best_params = knn_classifier_grid_search.best_params_
print("Best Hyperparameters:", knn_classifier_best_params)
knn_classifier_best=knn_classifier_grid_search.best_estimator_

decision_tree_classifer=DecisionTreeClassifier()

decision_tree_param_grid = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

decision_tree_grid_search = GridSearchCV(estimator=decision_tree_classifer, param_grid=decision_tree_param_grid, scoring='neg_log_loss', cv=5)
decision_tree_grid_search.fit(x_train,y_train)

decision_tree_best_params = decision_tree_grid_search.best_params_
print("Best Hyperparameters:", decision_tree_best_params)
decision_tree_classifier_best=decision_tree_grid_search.best_estimator_

random_forest_classifer=RandomForestClassifier()

random_forest_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

random_forest_grid_search = GridSearchCV(estimator=random_forest_classifer, param_grid=random_forest_param_grid, scoring='neg_log_loss', cv=5)
random_forest_grid_search.fit(x_train,y_train)

random_forest_best_params = random_forest_grid_search.best_params_
print("Best Hyperparameters:", random_forest_best_params)

random_forest_classifier_best=random_forest_grid_search.best_estimator_

mlp_prd=mlp_cls.predict(x_test)
mlp_prd=mlp_prd.round()

xgb_crs_val_scr=cross_val_score(xgb_cls_bst,x_train,y_train,scoring='accuracy')
xgb_crs_val_scr

xgb_prd=xgb_cls_bst.predict(x_test)

random_forest_cross_validation_score=cross_val_score(random_forest_classifier_best,x_train,y_train,scoring='accuracy')
random_forest_cross_validation_score

random_forest_prediction=random_forest_classifier_best.predict(x_test)

decision_tree_cross_validation_score=cross_val_score(decision_tree_classifier_best,x_train,y_train,scoring='accuracy')
decision_tree_cross_validation_score

decision_tree_prediction=decision_tree_classifier_best.predict(x_test)

logistic_regression_cross_validation_score=cross_val_score(logistic_regression_classifer,x_train,y_train,scoring='accuracy')
logistic_regression_cross_validation_score

logistic_regression_prediction=logistic_regression_classifer.predict(x_test)

knn_cross_validation_score=cross_val_score(knn_classifier_best,x_train,y_train,scoring='accuracy')
knn_cross_validation_score

knn_prediction=knn_classifier_best.predict(x_test)

mlp_tst_ls,mlp_accr=mlp_cls.evaluate(x_test,y_test)

mlp_cls_cnfsn_mtrx=confusion_matrix(y_test,mlp_prd)
ConfusionMatrixDisplay.from_predictions(y_test,mlp_prd)
plt.title('Confusion Matrix of MLP')
plt.show()

mlp_acc=accuracy_score(y_test,mlp_prd)
mlp_prcsn=precision_score(y_test,mlp_prd)
mlp_rcl=recall_score(y_test,mlp_prd)
mlp_spc=mlp_cls_cnfsn_mtrx[0][0]/(mlp_cls_cnfsn_mtrx[0][0]+mlp_cls_cnfsn_mtrx[0][1])

print("Accuracy of MLP: ",mlp_acc)
print("Precision of MLP: ",mlp_prcsn)
print("Recall of MLP: {}".format(mlp_rcl))
print("Specificity of MLP: ",mlp_spc)

xgb_cls_cnfsn_mtrx=confusion_matrix(y_test,xgb_prd)
ConfusionMatrixDisplay.from_predictions(y_test,xgb_prd)
plt.title('Confusion Matrix of XGBoost')
plt.show()

xgb_acc=accuracy_score(y_test,xgb_prd)
xgb_prcsn=precision_score(y_test,xgb_prd)
xgb_rcl=recall_score(y_test,xgb_prd)
xgb_spc=xgb_cls_cnfsn_mtrx[0][0]/(xgb_cls_cnfsn_mtrx[0][0]+xgb_cls_cnfsn_mtrx[0][1])

print("Accuracy of XGBoost: ",xgb_acc)
print("Precision of XGBoost: ",xgb_prcsn)
print("Recall of XGBoost: {}".format(xgb_rcl))
print("Specificity of XGBoost: ",xgb_spc)

logistic_regression_confusion_matrix=confusion_matrix(y_test,logistic_regression_prediction)
ConfusionMatrixDisplay.from_predictions(y_test,logistic_regression_prediction)
plt.title('Confusion Matrix of Logistic Regression')
plt.show()

logistic_regression_accuracy=accuracy_score(y_test,logistic_regression_prediction)
logistic_regression_precision=precision_score(y_test,logistic_regression_prediction)
logistic_regression_recall=recall_score(y_test,logistic_regression_prediction)
logistic_regression_specificity=logistic_regression_confusion_matrix[0][0]/(logistic_regression_confusion_matrix[0][0]+logistic_regression_confusion_matrix[0][1])

print("Accuracy of Logistic Regression: ",logistic_regression_accuracy)
print("Precision of Logistic Regression: ",logistic_regression_precision)
print("Recall of Logistic Regression: ",logistic_regression_recall)
print("Specificity of Logistic Regression: ",logistic_regression_specificity)

knn_confusion_matrix=confusion_matrix(y_test,knn_prediction)
ConfusionMatrixDisplay.from_predictions(y_test,knn_prediction)
plt.title('Confusion Matrix of KNN')
plt.show()

knn_accuracy=accuracy_score(y_test,knn_prediction)
knn_precision=precision_score(y_test,knn_prediction)
knn_recall=recall_score(y_test,knn_prediction)
knn_specificity=knn_confusion_matrix[0][0]/(knn_confusion_matrix[0][0]+knn_confusion_matrix[0][1])

print("Accuracy of KNN: ",knn_accuracy)
print("Precision of KNN: ",knn_precision)
print("Recall of KNN: ",knn_recall)
print("Specificity of KNN: ",knn_specificity)

decision_tree_confusion_matrix=confusion_matrix(y_test,decision_tree_prediction)
ConfusionMatrixDisplay.from_predictions(y_test,decision_tree_prediction)
plt.title('Confusion Matrix of Decision Tree')
plt.show()

decision_tree_accuracy=accuracy_score(y_test,decision_tree_prediction)
decision_tree_precision=precision_score(y_test,decision_tree_prediction)
decision_tree_recall=recall_score(y_test,decision_tree_prediction)
decision_tree_specificity=decision_tree_confusion_matrix[0][0]/(decision_tree_confusion_matrix[0][0]+decision_tree_confusion_matrix[0][1])

print("Accuracy of Decision Tree: ",decision_tree_accuracy)
print("Precision of Decision Tree: ",decision_tree_precision)
print("Recall of Decision Tree: ",decision_tree_recall)
print("Specificity of Decsion Tree: ",decision_tree_specificity)

random_forest_confusion_matrix=confusion_matrix(y_test,random_forest_prediction)
ConfusionMatrixDisplay.from_predictions(y_test,random_forest_prediction)
plt.title('Confusion Matrix of Random Forest')
plt.show()

random_forest_accuracy=accuracy_score(y_test,random_forest_prediction)
random_forest_precision=precision_score(y_test,random_forest_prediction)
random_forest_recall=recall_score(y_test,random_forest_prediction)
random_forest_specificity=random_forest_confusion_matrix[0][0]/(random_forest_confusion_matrix[0][0]+random_forest_confusion_matrix[0][1])

print("Accuracy of Random Forest: ",random_forest_accuracy)
print("Precision of Random Forest: ",random_forest_precision)
print("Recall of Random Forest: ",random_forest_recall)
print("Specificity of Random Forest: ",random_forest_specificity)

xgb_perm=PermutationImportance(xgb_cls_bst).fit(x_test,y_test)
show_weights(xgb_perm,feature_names=X_over.columns.tolist())

#prds = mlp_cls.predict(x_test)
#rnd_prds = np.round(prds)
#mlp_perm=PermutationImportance(mlp_cls, scoring='neg_log_loss').fit(x_test,y_test)
#show_weights(mlp_perm,feature_names=X_over.columns.tolist())

#decision_tree_perm=PermutationImportance(decision_tree_classifer).fit(x_test,y_test)
#show_weights(decision_tree_perm,feature_names=X_over.columns.tolist())

random_forest_perm=PermutationImportance(random_forest_classifier_best).fit(x_test,y_test)
show_weights(random_forest_perm,feature_names=X_over.columns.tolist())

logistic_regression_perm=PermutationImportance(logistic_regression_classifer).fit(x_test,y_test)
show_weights(logistic_regression_perm,feature_names=X_over.columns.tolist())

#knn_perm=PermutationImportance(knn_classifier).fit(x_test,y_test)
#show_weights(knn_perm,feature_names=X_over.columns.tolist())

RocCurveDisplay.from_predictions(y_test,random_forest_prediction)

RocCurveDisplay.from_predictions(y_test,logistic_regression_prediction)

RocCurveDisplay.from_predictions(y_test,mlp_prd)

RocCurveDisplay.from_predictions(y_test,xgb_prd)

RocCurveDisplay.from_predictions(y_test,decision_tree_prediction)

RocCurveDisplay.from_predictions(y_test,knn_prediction)

joblib.dump(random_forest_classifier_best, '/content/drive/MyDrive/random_forest_model.joblib')

X2_over = X_over.drop(['hypertension'],axis=1)
X2_over.head()

standard_scalar2=StandardScaler()
normalized_data2=standard_scalar2.fit_transform(X2_over)

x3_train,x3_test,y3_train,y3_test=train_test_split(normalized_data2,y_over,shuffle=True)

xgb_prm_grd2 = {
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
}

xgb_grd_srch2 = GridSearchCV(estimator=xgb_cls, param_grid=xgb_prm_grd2, scoring='neg_log_loss', cv=5)
xgb_grd_srch2.fit(x3_train, y3_train)

xgb_bst_prms2 = xgb_grd_srch2.best_params_
print("Best Hyperparameters:", xgb_bst_prms2)
xgb_cls_bst2=xgb_grd_srch2.best_estimator_

xgb_crs_val_scr2=cross_val_score(xgb_cls_bst2,x_train,y_train,scoring='accuracy')
xgb_crs_val_scr2

xgb_prd2=xgb_cls_bst2.predict(x3_test)

xgb_cls_cnfsn_mtrx2=confusion_matrix(y3_test,xgb_prd2)
ConfusionMatrixDisplay.from_predictions(y3_test,xgb_prd2)
plt.title('Confusion Matrix of XGBoost')
plt.show()

xgb_acc2=accuracy_score(y3_test,xgb_prd2)
xgb_prcsn2=precision_score(y3_test,xgb_prd2)
xgb_rcl2=recall_score(y3_test,xgb_prd2)
xgb_spc2=xgb_cls_cnfsn_mtrx2[0][0]/(xgb_cls_cnfsn_mtrx2[0][0]+xgb_cls_cnfsn_mtrx2[0][1])

print("Accuracy of XGBoost: ",xgb_acc2)
print("Precision of XGBoost: ",xgb_prcsn2)
print("Recall of XGBoost: {}".format(xgb_rcl2))
print("Specificity of XGBoost: ",xgb_spc2)

mlp_cls2=Sequential([
    Dense(units=512,activation='relu',input_dim=x3_train.shape[1]),
    Dropout(0.1),
    Dense(units=256,activation='relu'),
    Dropout(0.1),
    Dense(units=128,activation='relu'),
    Dropout(0.1),
    Dense(units=64,activation='relu'),
    Dropout(0.1),
    Dense(units=1,activation='sigmoid')
])

x4_train, x4_val, y4_train, y4_val = train_test_split(x3_train, y3_train, test_size=0.2, shuffle=True)
erl_stpg=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)

mlp_cls2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
mlp_cls2.fit(x4_train,y4_train,epochs=100,batch_size=256,validation_data=(x4_val,y4_val),callbacks=[erl_stpg])

mlp_prd2=mlp_cls2.predict(x3_test).round()
mlp_tst_ls2,mlp_accr2=mlp_cls2.evaluate(x3_test,y3_test)

mlp_cls_cnfsn_mtrx2=confusion_matrix(y3_test,mlp_prd2)
ConfusionMatrixDisplay.from_predictions(y3_test,mlp_prd2)
plt.title('Confusion Matrix of MLP')
plt.show()

mlp_acc2=accuracy_score(y3_test,mlp_prd2)
mlp_prcsn2=precision_score(y3_test,mlp_prd2)
mlp_rcl2=recall_score(y3_test,mlp_prd2)
mlp_spc2=mlp_cls_cnfsn_mtrx2[0][0]/(mlp_cls_cnfsn_mtrx2[0][0]+mlp_cls_cnfsn_mtrx2[0][1])

print("Accuracy of MLP: ",mlp_acc2)
print("Precision of MLP: ",mlp_prcsn2)
print("Recall of MLP: {}".format(mlp_rcl2))
print("Specificity of MLP: ",mlp_spc2)

